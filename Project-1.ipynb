{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring the positions as column names for the DNA squence after splitting it.\n",
    "column_names=['p1','p2','p3','p4','p5','p6','p7','p8','p9','p10',\n",
    "                        'p11','p12','p13','p14','p15','p16','p17','p18','p19','p20',\n",
    "                        'p21','p22','p23','p24','p25','p26','p27','p28','p29','p30',\n",
    "                        'p31','p32','p33','p34','p35','p36','p37','p38','p39','p40',\n",
    "                        'p41','p42','p43','p44','p45','p46','p47','p48','p49','p50',\n",
    "                        'p51','p52','p53','p54','p55','p56','p57','p58','p59','p60','label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_training_data():\n",
    "    \"\"\"\n",
    "    Read the DNA sequence and labels from the CSV file\n",
    "    Split the sequence into individual characters\n",
    "    Assign new column names as positions into the training data\n",
    "    \"\"\" \n",
    "    # Read the DNA sequence and their classes from file training.csv into a dataframe.\n",
    "    df = pd.read_csv('training.csv',header=None,  usecols=[1,2])\n",
    "    df.columns=['data','label']\n",
    "    \n",
    "    # Split the sequence as individual postions into a new dataframe and assigning the column names declared to it.\n",
    "    training_data_frame = pd.DataFrame()\n",
    "    training_data_frame=df['data'].apply(lambda k: pd.Series(list(k)))                   \n",
    "    training_data_frame[60] = df['label']\n",
    "    training_data_frame.columns=column_names\n",
    "    \n",
    "    # Return the data frame.\n",
    "    return training_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_testing_data():\n",
    "    \"\"\"\n",
    "    Read the DNA sequence from the CSV file\n",
    "    Split the sequence into individual characters\n",
    "    Assign new column names as positions into the testing data\n",
    "    \"\"\"\n",
    "    # Read the DNA sequence from file testing.csv into a dataframe.\n",
    "    df = pd.read_csv('testing.csv',header=None,  usecols=[1])\n",
    "    df.columns=['data']\n",
    "    \n",
    "    # Split the test DNA sequence as individual postions into a new dataframe and assigning the corresponding column names declared to it.\n",
    "    testing_data_frame = pd.DataFrame()\n",
    "    testing_data_frame=df['data'].apply(lambda k: pd.Series(list(k)))\n",
    "    testing_data_frame.columns=column_names[0:60]\n",
    "    \n",
    "    # Return the data frame.\n",
    "    return testing_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_misc_error(labels_column):\n",
    "    \"\"\"\n",
    "    Calculates the Miscalculation Error at a node using below argument.\n",
    "      1.labels_column = The entire labels column from the current dataset at that node\n",
    "    \"\"\"\n",
    "    # Getting the unique labels and their counts in the label column.\n",
    "    labels,total_number = np.unique(labels_column,return_counts=True)\n",
    "    \n",
    "    # Intializing the misc_error with value 1 and max_label_ratio with 0.\n",
    "    misc_error=1\n",
    "    max_label_ratio=0\n",
    "    \n",
    "    # Finding the max_label_ratio from the array of unique labels.\n",
    "    for i in range(len(labels)):\n",
    "        max_label_ratio = np.maximum(max_label_ratio,(total_number[i]/np.sum(total_number)))\n",
    "    \n",
    "    # Returning miscellaneous calculation error.\n",
    "    return misc_error-max_label_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(labels_column):\n",
    "    \"\"\"\n",
    "    Calculates the Entropy at a node using below argument.\n",
    "      1.labels_column = The entire labels column from the current dataset at that node\n",
    "    \"\"\"\n",
    "    # Getting the unique labels and their counts in the label column.\n",
    "    labels,total_number = np.unique(labels_column,return_counts=True)\n",
    "    \n",
    "    # Intializing the entropy with value 0.\n",
    "    entropy=0\n",
    "    \n",
    "    # Evaluating the entropy for the array of labels and their corresponding counts.\n",
    "    for i in range(len(labels)):\n",
    "        entropy = entropy + (-total_number[i]/np.sum(total_number))*np.log2(total_number[i]/np.sum(total_number))\n",
    "    \n",
    "    # Returning entropy.\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini_index(labels_column):\n",
    "    \"\"\"\n",
    "    Calculates the Gini Index at a node using below argument.\n",
    "      1.labels_column = The entire labels column from the current dataset at that node\n",
    "    \"\"\"\n",
    "     # Getting the unique labels and their counts in the label column.\n",
    "    labels,total_number = np.unique(labels_column,return_counts=True)\n",
    "    \n",
    "    # Intializing the entropy with value 0.\n",
    "    gini_index=1\n",
    "    \n",
    "    # Evaluating the entropy for the array of labels and their corresponding counts.\n",
    "    for i in range(len(labels)):\n",
    "        gini_index = gini_index - np.square(total_number[i]/np.sum(total_number))*np.log2(total_number[i]/np.sum(total_number))\n",
    "    \n",
    "    # Returning entropy.\n",
    "    return gini_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(data,splitting_attribute, impurity_method):\n",
    "    \"\"\"\n",
    "    Calculates the Information Gain of the dataset on the splitting attribute using the below parameters.\n",
    "     1.data= dataset for the Information gain needs to be calculated.\n",
    "     2.splitting_attribute = the attribute on which dataset is split.\n",
    "     3.impurity_method = method used to calcuate the impurity for the data splitting.\n",
    "    \"\"\"\n",
    "    # Based on the input impurity_method parameter, setting the local impurity_method to misc_error or entropy or gini_index   \n",
    "    if(impurity_method=='misc_error'):\n",
    "        impurity_method=calculate_misc_error\n",
    "    elif(impurity_method=='entropy'):\n",
    "        impurity_method=calculate_entropy\n",
    "    else:\n",
    "        impurity_method=calculate_gini_index\n",
    "     \n",
    "    # Evaluate the impurity for the splitting attaribute using dataset and current impurity method.\n",
    "    impurity_at_splitting_attribute = impurity_method(data['label'])\n",
    "    \n",
    "    # Getting the unique values and corrsponding counts for the splitting attribute.\n",
    "    split_attribute_values,total_number = np.unique(data[splitting_attribute],return_counts=True)\n",
    "    \n",
    "    # Evaluating the sum of impurities on the child nodes after splitting on the split_attribue.\n",
    "    total_impurity_after_split = np.sum([(total_number[i]/np.sum(total_number))*impurity_method(data.where(data[splitting_attribute]==split_attribute_values[i]).dropna()['label']) for i in range(len(split_attribute_values))])\n",
    "    \n",
    "    # Returning entropy as difference between impurity at splitting_attribute and the sum at its child nodes.\n",
    "    return impurity_at_splitting_attribute-total_impurity_after_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3_algorithm(current_data,original_dataset,attributes,impurity_method,source_node_label=None):\n",
    "    \"\"\"\n",
    "    Builds the Decision Tree using the below parameters.\n",
    "     1.current_data= Dataset at the current node.\n",
    "     2.original_dataset = The original dataset for the whole decision tree. \n",
    "     3.attributes = The list of attributes available in the current dataset. \n",
    "     4.impurity_method = Method used to calcuate the impurity for the data splitting.\n",
    "     5.source_node_label = Most frequent label at the source node.\n",
    "    \"\"\"\n",
    "    # If there is only one label present in the current dataset, then return it.\n",
    "    if len(np.unique(current_data['label'])) <= 1:\n",
    "        return np.unique(current_data['label'])[0]\n",
    "    \n",
    "    # If the current dataset is empty, then return the most frequent label from the original dataset.\n",
    "    elif len(current_data)==0:\n",
    "        return np.unique(original_dataset['label'])[np.argmax(np.unique(original_dataset['label'],return_counts=True)[1])]\n",
    "    \n",
    "    # If there are no more attributes present, then return the most frequent label under the source node of current node.\n",
    "    elif len(attributes) ==0:\n",
    "        return source_node_label\n",
    "    else:\n",
    "    \n",
    "        # Calculate the most frequent label at the current node.\n",
    "        source_node_label = np.unique(current_data['label'])[np.argmax(np.unique(current_data['label'],return_counts=True)[1])]\n",
    "        \n",
    "        # Calculate the Information Gain for all the attributes using the Impurity Method specified.\n",
    "        if impurity_method=='misc_error':\n",
    "            attributes_IG_values = [information_gain(current_data,attribute,'misc_error') for attribute in attributes] \n",
    "        elif impurity_method=='gini_index':\n",
    "            attributes_IG_values = [information_gain(current_data,attribute,'gini_index') for attribute in attributes]\n",
    "        else:\n",
    "            attributes_IG_values = [information_gain(current_data,attribute,'entropy') for attribute in attributes]\n",
    "        \n",
    "        # Find the attribute with higest IG value in the array.\n",
    "        optimal_split_attribute_index = np.argmax(attributes_IG_values)\n",
    "        optimal_split_attribute = attributes[optimal_split_attribute_index]      \n",
    "        \n",
    "        # Create a new tree with root node as optimal splitting attribute.\n",
    "        decision_tree = {optimal_split_attribute:{}}  \n",
    "        \n",
    "        # Set the most frequent label for the root node in the decision tree.\n",
    "        decision_tree['max']=source_node_label\n",
    "        \n",
    "        # Perform Chi-square test to check if it is optimal to expand the decision tree.\n",
    "        if(chi_square(current_data,optimal_split_attribute,'label',0.99)):\n",
    "            \n",
    "            # Remove the splitting attribute from the attribute list.\n",
    "            attributes = [i for i in attributes if i != optimal_split_attribute]\n",
    "            \n",
    "            # For each of the values of the splitting attribute, create a new decision tree using and set them as child nodes in form of key-value pairs.\n",
    "            for split_attribute_value in np.unique(current_data[optimal_split_attribute]):\n",
    "                value = split_attribute_value\n",
    "                new_dataset = current_data.where(current_data[optimal_split_attribute] == value).dropna()            \n",
    "                sub_decision_tree = ID3_algorithm(new_dataset,original_dataset,attributes,impurity_method,source_node_label)            \n",
    "                decision_tree[optimal_split_attribute][value] = sub_decision_tree\n",
    "        \n",
    "        # Return the decision tree.\n",
    "        return(decision_tree)   \n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square(dataset, attribute_name, label_attribute_name,confidence):\n",
    "    \"\"\"\n",
    "    Performs a chi-square test on the dataset and given attribute using the below parameters. \n",
    "    Returns boolean whether it is optimal to perform split on the given attribute or not.\n",
    "     1.dataset= Dataset at the current node.\n",
    "     2.attribute_name = The attribute on which we are performing the test. \n",
    "     3.label_attribute_name = The attribute name of the label column. \n",
    "     4.confidence = Confidence level currently used in the test.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Getting the unique values and their corresponding counts for the chi-square test attribute.\n",
    "    attribute_values,counts_attr_values = np.unique(dataset[attribute_name],return_counts=True)\n",
    "    \n",
    "    # Getting the unique labels and their corresponding counts in the dataset.\n",
    "    label_attribute_values,counts_label_values=np.unique(dataset[label_attribute_name],return_counts=True)\n",
    "    \n",
    "    # Setting the intial chi-value to 0.\n",
    "    actual_chi_value=0;\n",
    "    \n",
    "    # Calculating the actual chi-value \n",
    "    for i in range(len(attribute_values)):\n",
    "        for j in range(len(label_attribute_values)):\n",
    "            real_count= dataset.loc[(dataset[attribute_name]==attribute_values[i]) & (dataset[label_attribute_name]==label_attribute_values[j])].shape[0]\n",
    "            expected_count = (counts_attr_values[i]*counts_label_values[j])/np.sum(counts_label_values)\n",
    "            actual_chi_value= actual_chi_value + (np.square(real_count-expected_count)/expected_count)\n",
    "    \n",
    "    # Evaluating the degrees of freedom for the given attribute and dataset.\n",
    "    dof = (len(attribute_values)-1)*(len(label_attribute_values)-1)\n",
    "    \n",
    "    # Calculating the expected chi-value.\n",
    "    expected_chi_value = chi2.ppf(confidence,dof)\n",
    "    \n",
    "    # Check if the actual value is greater or equal to expected and return.\n",
    "    return actual_chi_value>=expected_chi_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sequence,decision_tree):\n",
    "    \"\"\"\n",
    "    Classifies the DNA sequence using the below parameters.\n",
    "     1. sequence = Input test DNA sequence.\n",
    "     2. decision_tree = Decision Tree Build using the Training Data.\n",
    "    \"\"\"\n",
    "    # Keep navigating down the decision through the branches having values same as test DNA sequence until a label is reached.\n",
    "    # If the value is not found at any position in the tree, the return the most frequent label at that node as label for that sequence.\n",
    "    while (isinstance(decision_tree,dict) and not isinstance(decision_tree,str)):\n",
    "        try:\n",
    "            root = list(decision_tree.keys())[0]\n",
    "            decision_tree=decision_tree[root][sequence[root]] \n",
    "        except:\n",
    "            decision_tree = decision_tree['max']\n",
    "    # Return classified label.\n",
    "    return decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_test(test_dataset,decision_tree):\n",
    "    \"\"\"\n",
    "    Performs the classifcation test on the test dataset and writes the output to a file.\n",
    "     1.test_dataset = Dataset with all the test DNA sequences\n",
    "     2. decision_tree = Decision Tree Build using the Training Data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Converting the test DNA sequences to key-value pairs.\n",
    "    sequences = test_dataset.iloc[:,:].to_dict(orient = \"records\")\n",
    "    \n",
    "    # Classifing each sequence and write them to a file results.csv in the required format.\n",
    "    df = pd.DataFrame(columns=['id','class'])\n",
    "    for i in range(len(test_dataset)):\n",
    "        df.at[i,'id']=2001+i\n",
    "        df.at[i,'class']=  classify(sequences[i],decision_tree) \n",
    "    df.to_csv('results.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Training Dataset\n",
    "training_dataset=read_training_data()\n",
    "\n",
    "# Build a decision tree using the training data.\n",
    "decision_tree = ID3_algorithm(training_dataset,training_dataset,training_dataset.columns[:60],'entropy')\n",
    "\n",
    "# Read Test Dataset\n",
    "test_dataset = read_testing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform classification test on the test dataset.\n",
    "classification_test(test_dataset,decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
